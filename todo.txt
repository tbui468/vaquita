    Efficiency of dbms can be estimated with number IO operations
    Use gdb to walk through code (Cherno recommended debugger to learn to read code)
        https://www.cs.yale.edu/homes/aspnes/pinewiki/C(2f)Debugging.html
        Try it on the xv6 (or the risc-v version) - would gdb even work with vm?

    Depending on record sizes in a given leaf, serialize to / deserialize from multiple data blocks 
        before serializing leaf, check size of all records (fixed and variable length, including offset)
        and allocate (if not already allocated) enough data blocks for leaf

        node_required_data_blocks(leaf) should return the number of required data blocks

    Rewrite de/serialization of leaf node to use multiple data buffers (in case of node overflow)
        both serialization and deserializtion of leaves will require a list of buffers (since data nodes can overflow)
        struct VdbBuffer {
            uint8_t* buf;
            uint32_t size; <------fixed to page size
        }
        struct VdbBufferList {
            struct VdbBuffer* bufs;
            uint32_t count;
            uint32_t capacity;
        };

    Should store schema in struct Tree since we need it quite a bit, and having to open metadata each time is annoying

    void vdb_delete_record(VDBHANDLE h, const char* table, uint32_t key)
        don't worry about reusing keys for now
        set the key to 0 (since primary keys will start from 1) to signify free record
        set key to next pointer
        may need to serialize record size (aren't doing that right now)
        set count to size of record (including key and count)

        block header should hold ptr to first free record
            Need to load this into tree data structures when reading in a block
        
        Each serialized record may need: next, key, size (includes next, key, size and record data), record data ...
            When a record is deleted, use next as a pointer to next free record

        Inserting records now may reuse an old key that had its data record deleted

    void vdb_update_record(VDBHANDLE h, const char* table, uint32_t key, struct VdbRecord* rec);
        this becomes problematic with variable length data (eg, strings)
        what happens if the block doesn't have enough space???

    Write code to insert/fetch a ton of records for:
        1. testing if insert and fetch actually work
        2. measure efficiency of program (so that future changes can measure quality)

    System should not write blocks when release
        change functions to: tree_pin_node and tree_unpin_node (rather than catch and release)
        each block should have a pin field that is incremented for each time it's caught
        decrement the pin count by one each time it's released
        also have a dirty field set to true if the contents of the block are modified
        when the pager needs to evict a page, check if pin count == 0 before evicting
        write to disk if page is evicted and dirty bit is set

    Basic query to get range of records (eg, keys between 4 and 10)
        SELECT * FROM students
        SELECT name, age FROM students
        SELECT * FROM students WHERE age < 10

    Look up relational algebra predicates and implement basic queries
        projection (SELECT)
        selection (WHERE)
       
    Logger module
        Use a write-ahead log to make sure database doesn't become corrupt if the system fails (crash, electricty dies, etc) while writing 

    Note: Need to release all nodes before we start modifying a tree (this was problem with why split_leaf was not working)
        if not, then releasing old nodes later may overwrite changes

        How about this: when a node is current used, set a flag somewhere in it
            if this flag is set, assert false so that we know that we know that it's a problem

    Deleting records
        Should keys be reused?  How does sqlite do it? - sqlite allows reuse if user desires

    Switch to binary search in _tree_traverse_to_leaf (just using linear scan right now)

    Limit how many blocks are in buffer cache at a time - use least-recently used eviction policy

    Refactor vdb.c to make it more readable - it's a bit messy now

    Allow fetch to get range of records by primary key - first step into creating an execution engine here

    How to implement execution engine:
        SQL parser and VM to run the bytecode

