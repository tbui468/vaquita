    Efficiency of dbms can be estimated with number IO operations

    Use gdb to walk through code (Cherno recommended debugger to learn to read code)
        https://www.cs.yale.edu/homes/aspnes/pinewiki/C(2f)Debugging.html
        Try it on the xv6 (or the risc-v version) - would gdb even work with vm?

    https://github.com/firstcontributions/first-contributionsool
        go to this link and do a basic contribution

    Use ChatGPT to help write paper on neural network to learn
        eviction policy for pager - integration of neural networks
        into legacy code

    check out nandgame for website on how to create processor from scratch
        then try to do it using verilog

    Use Yahoo or some other database standard metric to focus on core features

    cstack.github.io for short guide to basics of SQL implementation (not complete)

    Read Edward Sciore's book on implementing a relational database
        Chapter 4: Memory Management (PDF 92)

    Use SDL and Vulkan to visualize stars/planets/moons in galaxy

    sqlite documentation has great explanation of how bytecode works

    handmade.network has cool mini projects (such as barebones OS with only a few thousand lines of C)

    OS
        James Molloy has a good website for OS dev
        use qemu
        use OS dev so that we can set up skeleton project

    Look here for two star databases:
        https://github.com/astronexus/HYG-Database
        One has around 100,000 stars, one has about 200,000 stars/objects and one database has over 2.5 billion(!) stars

    valgrind --leak-check=full --show-leak-kinds=all --log-file="output.txt" -v ./../build/src/vdb sol.sql

***********************Documentation*****************************
Example
Major components
Todos

*************************Performance***************

Takes about 1:40 to lex/parse inserting 10,000 records
Takes about 1:40 to execute the AST for 10,000 records

*******************************TODO**************************************************
    vdbhashtable_insert_entry_by_grouping(grouping_table, rec, 0) <---- IMPLEMENT THIS
        currently has a HUGE bug - we are assuming NO collisions in hashes - but there's no reason for this to be true
        rather than having a single terminal record set, need to check ALL groupby values and see if (use vdbrecord_concat_values)
        they are truly the same.  If yes, insert into existing record set.  If not, need to create new record set.

        How about this:
            struct VdbRecordSet can hold either a struct VdbRecord or struct VdbRecordGroup
                this will allow the terminal recordset to contain a list of records/record groups
                but then binarytrees would need to be rewritten since they produce record sets when flattened


        struct VdbTupleSet - final set of tuples.  The result of (sub)query with all projections/aggregates applied
        struct VdbRecordSet - intermediate data before projection/aggregates applied
        struct VdbRecordSetList - need this in hash table to store potentially multiple groups within same bucket (collisions)
        ???Think about this some more.....

        struct VdbRecordGroup {
            struct VdbRecordSet* sets;
            int count;
            int capacity;
        };


    group by
        struct VdbIntList* il = vdbcursor_attrs_to_idxs(cursor, stmt->as.select.grouping);
        struct VdbHashTable* ht = vdbhashtable_init(fcn, il); //accepts hash function as argument
        //grab records that fit selection
            vdbhashtable_insert_entry(ht, rec);
        struct VdbRecordSet* rs = vdbhashtable_concat_tables(ht);

        The final level of the nested hash tables should hold a single record set fufilling group by requirements
            just pick the first record for project, and use the entire record set for any aggregation

        select * from stars group by moons;
            this should return ONE record from each moon count
            aggregates can then be used with ANy column
            select avg(moons) from stars group by moons; //display number of moons, doesn't really do anything useful
            select avg(gravity), max(mass), moons from stars group by moons;

        distinct and group by don't really go well together...
            select distinct age, name from students group by grade (but just do group by first)
            group by should go first, and then apply the distinct projection

        group by allows use of count/avg/sum/min/max on non-group columns
            select avg(grade) from students group by gender;

            min/max
                select max(dist) from stars where dist < 1000.0;
                Toss these records into the binary tree used in 'order by' too - then just grab what we need at the end

            count/avg/sum
                select count(dist) from stars where dist < 1000.0;
                Use a few variables to keep a running total of all count and sum
                Then compute average at end if needed

        group by also allows having keyword (would this query work?)
            where cannot be used with aggregates, so having was included
            select name, grade from students group by gender having avg(grade) > 50;

    subqueries
        select distinct name from (select * from planets order by name desc);
        what other subqueries could we use as tests?

        Records are tuples of data
        A record is the entire tuple of data
        A tuple is a (possible) subset of the record

    ***Big Rewrite***
    Switch to new B-Tree format: have leaves hold ptrs to data blocks (which can now hold arbitrarily large records, so strings can fit)
        This allows multiple indexes in the same file more easily since they can all reference same data blocks
        For now, don't cluster data - just have it appended to currently free data blocks
        Make indexes work correctly with primary key:
            where id = 10, id != 10, id < 10, id > 10 
                How do we optmize cursor postion for these?  Is there a general query plan for all these cases?
                    Where does the cursor start?
                    Where does it end?
                    How many cursors do we need?

            Since indexing only slows down inserts/updates/deletes, we could insert everything, and then
                index ALL columns to speed up select queries

    limit
        select * form stars where dist < 1000.0 order by dist asc limit 10;

    copy stmt
        *this should require the csv to be cleaned, but it'll be faster than parsing and executing SQL for all the inserts
        copy stars(id, proper, dist) from "hyb_30.csv" delimiter ',' csv header;

    Compiler may make insertion faster
        codegen.c - spits out bytecode
        vm.c - runs vm

    Load up a large database so that buffer pool can be tested
        generate script to:
            create database and table first (drop if existing)
            then add about 10 stars
            then 100
            then 1000
            then 10000
            then entire file (10000s)

    Eviction policy
        make db really big so that the program crashes because of too much memory allocated, then evict pages to fix it
        Or just manually limit number of pages allowed to hold at a time
        Use O_DIRECT to bypass OS pager

        implement LRU policy for now, but make it easy to swap algorithms (such as to clock policy or using a nn)
        make a linked list of currently cached blocks
        define MAX_BLOCKS 4
        when we need a block, scan the list and look for it
        if not found, remove last block (flush if necessary) and load in new block from disk 

    tree.c/cursor (in vdb.c) needs a massive rewrite - it's too confusing now
        start with caching the schema instead of dynamically allocating/freeing it in every tree function
        abstraction between cursor and tree is fuzzy - what responsibility should each have?
        traverse_to_first_leaf is no longer necessary - remove it
    
        cursor functions are too light - it should really be calling most of the tree.c api to do what needs to be done
            tree functions should mainly deal with nodes (which the cursor shouldn't know about)

            abstraction layers: vm -> cursor -> tree -> node

            need to pull some complexity out of tree.c and into cursor.c

        cursor abstraction:
            cursors are created when starting a new transaction, and destroyed when transaction ends (commit or not)
            a cursor is associated with a table AND an ordering (eg, by which column the records are indexed)
            a cursor has a closed and open state:
                <open stmt> opens a cursor
                <close stmt>, <commit stmt>, or <rollback stmt> closes cursor
            cursor may be before first record or one past the last record
            fetch - moves cursor to next record and returns that record
            delete - deletes record pointed to by cursor - moves cursor to next record after deletion
            update - updates records pointed to by cursor

    allocated_memory variable show -52 bytes when runnging sol.sql
        I suspect that a non-wrapper allocation function is being called somewhere
        OR free_w has a wrong size argument

    Probably should cached schema and use that whenever vdbmeta_get_schema is called rather than having to
        manually free it every single time - error prone and just makes everything look messier

    User defined primary keys
        Need to make sure keys are unique
        Need to rewrite B-Tree to not always append at end

    Subqueries

    Add tests for errors:
        insert_invalid_col_name.sql <----should report error message.  Silenty failing now when mistyped, and hard to debug
            create table planets (name string, moons int);
            insert into planets (name, gravity) values ("Earth", 9.81); //this should report error instead of setting moons to null and ignoring 9.81

        *will probably need to introduce runtime errors here (we have tokenizing and parsing(?) errors already)

    single line comments
        use c-style //

    multiline comments
        use c-style /**/

    Don't need to specify column names if inserting all values
        insert into planets values (...), (...);

    If block size is 256 and block header size is 128, it crashes if not enough room to store schema data
        the attribute names take too much space.  Need an data blocks and overflow blocks for colunm names

    Compile AST to bytecode that runs on vm
        How could this work?
            what instructions are needed?
        when inserting, attribute names should be parsed as expressions (being parsed as single tokens now)
            can remove parse_identifier_tuple function then

            insert into planets (name, mass) values ("Mars", 242), ("Venus", 242), ("Earth", 534);

        
    need checks to prevent table manipulation/creation/dropping when database not open - segfaulting now

    need to return error if table doesn't exist when insert/select/update/delete - segfaulting now

    primary keys rework
        user MUST define primary key using 'primary key' keywords
        'auto' keyword will make an autoincrementing column value
        'constraint' can be used to set primary key
        should allow this:
            create table planets (name string, moons int);
            create table planets (name string not null, id int auto);
            create table planets (name string, moons int, primary key

    Concurrency - need to get read/write locks on file
        pager can take care of this

    Transactions - need a way of rolling back changes if transaction fails
        need a way to track undos (a stack???)

    Logging - database should never be in inconsistent state if system fails
        sqlite uses a write-ahead log


    Connect cli and the database tree to get useful data - play with MariaDB or MySQL to see how they print outputs
        > open school
            database school opened
        > select * from students
            +----+------+-----+
            | id | name | age |
            +----+------+-----+
            |  1 | John |  23 |
            |  2 | Kate |  12 |
            |  3 | Timm |  22 |
            +----+------+-----+
            3 rows in set
        > close school
            database school closed
            [what message?]

    Foreign keys to connect two or more tables
        joins will be needed here

    Custom keys
        Unique or not?
        Have to rewrite tree to split/merge based on where inserted (since it will not be in order anymore)

    views

    unique constraint

    not null constraint

    group by

    joins
    
    split into client and server - use tcp sockets to connect client to server
