    Efficiency of dbms can be estimated with number IO operations

    Use gdb to walk through code (Cherno recommended debugger to learn to read code)
        https://www.cs.yale.edu/homes/aspnes/pinewiki/C(2f)Debugging.html
        Try it on the xv6 (or the risc-v version) - would gdb even work with vm?

    https://github.com/firstcontributions/first-contributionsool
        go to this link and do a basic contribution

    Use ChatGPT to help write paper on neural network to learn
        eviction policy for pager - integration of neural networks
        into legacy code

    check out nandgame for website on how to create processor from scratch
        then try to do it using verilog

    Use Yahoo or some other database standard metric to focus on core features

    cstack.github.io for short guide to basics of SQL implementation (not complete)

    Read Edward Sciore's book on implementing a relational database
        Chapter 4: Memory Management (PDF 92)

    Use SDL and Vulkan to visualize stars/planets/moons in galaxy

    sqlite documentation has great explanation of how bytecode works

    handmade.network has cool mini projects (such as barebones OS with only a few thousand lines of C)

    OS
        James Molloy has a good website for OS dev
        use qemu
        use OS dev so that we can set up skeleton project

    Look here for two star databases:
        https://github.com/astronexus/HYG-Database
        One has around 100,000 stars, one has about 200,000 stars/objects and one database has over 2.5 million(!) stars

    valgrind --leak-check=full --show-leak-kinds=all --log-file="output.txt" -v ./../build/src/vdb sol.sql

    Walkthrough of git source code (cool!) 
        https://fabiensanglard.net/git_code_review/

    wyag.thb.lt
        writing version control software like git (pretty cool)

    Basic web server in python
        https://bhch.github.io/posts/2017/11/writing-an-http-server-from-scratch/

    Nginx Tutorial
        https://www.freecodecamp.org/news/the-nginx-handbook/

        /etc/nginx/nginx.config

        nginx (to start)
        service nginx status (to check status)
        nginx -s
            quit (shutdown gracefully)
            reload (reload configuration file)
            reopen (reopen log files)
            stop (shutdown immediately)
        nginx -t (check if config file is okay)

***********************Documentation*****************************
Example
Major components
Todos

*************************Performance***************
Takes ~5s to insert 119000 records from HYG

Try AHYG (expecting this to take around 2 minutes for 2.5 million records)
    need to push the limits of memory usage so that page eviction can be tested

Could try the entire galileo database with 1.7 billion records

*******************************TODO**************************************************
    Get flask running on nginx
        How to Use Nginx as a Reverse-Proxy (freecodecamp book) <---------START HERE
        Finish reading online book
        Get flask project running in nginx (with gunicorn)
            set this up the simpliest way possible - no need to worry about systemmd or other fancy stuff
            since we are only using this to run in development

    Finish README:
        build instructions
        running instructions (pick port number for server, and port number to connect to for client)
        examples using script (one is enough)
        examples using cli (show all currently supported features)
        describe current features
        features to come

    Clean up github
        delete ALL projects I don't want to share (really, just keep compilers/interpreters/solidity data serializer, neural network visualizer)

    Fix CV
        Make it ready for submission
            work: sorting and search algorithms
                  tetris, asteroids, pacman, battleship (in pygame)

            wedding website: ??? Should I put that on?

            Personal projects: neural network visualizer, compiler/linker for custom programming language, SQl implementation
            Languages: C/Python

    Finish implementing vdbnode_free_cell()
        add a freelist to header of nodes
        add datacell to freelist when a cell is freed

    Implement vdbnode_defrag()
        shift all free datacells to the right to remove gaps
        change idxcell values to reflect the newly shifted datacells
        return the newly allocated (or total?) free space

    Reimplement update
        add tests back in from github (2)
        need to free any variable length data if different size from updated value
        need to implement free of cells
        could probably also implement defragging of data blocks
            when should a data block be defragged (vacuumed)?
            whenever the current data block is full, and before a new data block is allocated,
            defrag blocks in list until one has enough space (also insert this block in in data block ptr in meta block)

    Reimplement delete
        add tests back in from github (2)
        delete should delete idxcell and add datacells to freelist (next field should be set)

    indexing:
        create index my_index on table_name (x, y);
        drop index my_index;

        To create index, go through primary key index and
            insert new nodes into new index.

        Meta nodes should also use idxcell and datacell format
            store index names/root as cells in meta node

    Make sure the cli works correctly
        make it more like mysql 

    Transactions
        If something fails part-way through, any changes should be reverted
        Need to log changes and undo if necessary

    Page eviction
        Use O_DIRECT and O_SYNC (along with switching from fopen to open)
            to see if bypassing OS cache will speed up queries

        Write open_w wrapper that uses O_DIRECT and O_SYNC
        Set a mamimum page count based on RAM (4-8 Gb seems reasonable)

    Logging
        use an append-only log to ensure data is not lost/corrupt if system
        goes down while writing.  Write to log first before writing from
        tree to disk

    Redo projection:
        Rather than an array of structs for recordsets, use a struct of arrays
        this will make projection easy - simply drop any arrays we don't need

    update will break if string is longer than current record size
        need to properly resize datacells if strings are updated
        need to store 3 data for each string: size, capacity, overflow

    Should NOT allow duplicate keys

    strings need work
        string keys are NOT currently saved in tree nodes correctly(the pointer /len is saved right now, which doensn't do anything or at worst breaks everything)
            put in some asserts to fail if a string is used as key, or fix this to allow strings as keys

        update will not work correctly if string fields are different sizes now...

    deleted datacells in leaf are not being reclaimed right now
        this can also be fixed when we switch over to data blocks to hold records

    select * from planets; //this should report error since there is no table called planets in universe db
        this was causing problems since I kept typing the wrong table name in (this shouldn't happend again)

    If table name is omitted, also report error

    Server should not require opaque handle anymore since user doesn't have direct access
        anymore

    Client functionality
        Opening multiple clients does NOT update database properly
            Each node is opening its own database handle with its own pager
            this shouldn't happen!  All clients should be using the same handle,
            pager, etc.  This is controlled by the server.  The client only can request
            the handle.  This is necessary since we need write/read locks later so that
            multiple nodes can't write at the same time.

            Need to copy-on-write root when a node wants to write...

        vm needs to handle own errors (lexer and parser need to handle errors better too)
            then add them to output (same as lexer and parser are currently doing)
            some code branches are hitting the asserts, which cause problems (program blocks until some bad shit happens)
                these should be errors that are recorded and sent to client

    network code needs to be pulled out into own module
        server.h/server.c for dbms server
        client.h (single header for easy drop-in) for client

    Combine windows and *nix versions into a single client header with ifdefs
        Write out windows version of client by hand to see what's in there first
        then abstract the windows/linux stuff into own functions, and common stuff into other functions
    Creating/dropping indexes:
        Indexes require multiple trees - we need to take records out of leaves and put them in own data blocks that
            all indices can refer to.  Could keep data nodes for variable length data, and make record nodes for fixed len record data

            Still 4 types of nodes:
                internal
                leaf
                record
                data

            in leaf, each index refers to record ptr (datacell).  Each record ptr has the node idx and idxcell idx.
            currently records are: [next|occupied|data...]
            new record data in leaf: [next|occupied|node_idx|idxcell_idx]

            datacell in data node: [next|

        Rewrite B-Tree so that:
            create index x_coords on planets.x; //only allow single indexing for now, so don't need planets(x) syntax
            drop index x_coords on planets;

            Any data type can be used as keys (string, int, (float as a key is stupid, but allow it), bool)
            leaves now only hold references to data blocks
            data blocks hold record data (including variable length data)
            data blocks can now be defragged if enough spac
            Need to be able to index by cartesian coordinates (x, y, z) so tha star app works

    Inserting records with out of order keys don't order them correctly in B-Tree (skip for now since B-Tree requires rewrite anyway)
        insert into planets (id, name, mass, atmosphere) values (5, "Neptune", 20, false), (1, "Mars", 10, true), (3, "Venus", 10, false);
        select * from planets;

        This should return Mars, Venus and then Neptune.  But it returns insertion order
        this will be need to change with rewriting of B-Tree, so just skip this for now

    joins
        cross-join
        inner-join (cross join with a condition)
        outer joins

        will need foreign keys here

    subqueries - seems like this is useful only with multiple tables (foreign keys may be needed too)
        select distinct name from (select * from planets order by name desc);
        what other subqueries could we use as tests?

        Records are tuples of data
        A record is the entire tuple of data
        A tuple is a subset of the record data (could possible be the entire thing too)
        
    copy stmt
        *this should require the csv to be cleaned, but it'll be faster than parsing and executing SQL for all the inserts
        copy stars(id, proper, dist) from "hyb_30.csv" delimiter ',' csv header;
        Essentially just make the AST for insert once, and then substitute

    Compiler may make insertion faster
        codegen.c - spits out bytecode
        vm.c - runs vm

    Load up a large database so that buffer pool can be tested
        generate script to:
            create database and table first (drop if existing)
            then add about 10 stars
            then 100
            then 1000
            then 10000
            then entire file (10000s)

    Eviction policy
        make db really big so that the program crashes because of too much memory allocated, then evict pages to fix it
        Or just manually limit number of pages allowed to hold at a time
        Use O_DIRECT to bypass OS pager

        implement LRU policy for now, but make it easy to swap algorithms (such as to clock policy or using a nn)
        make a linked list of currently cached blocks
        define MAX_BLOCKS 4
        when we need a block, scan the list and look for it
        if not found, remove last block (flush if necessary) and load in new block from disk 

    tree.c/cursor (in vdb.c) needs a massive rewrite - it's too confusing now
        start with caching the schema instead of dynamically allocating/freeing it in every tree function
        abstraction between cursor and tree is fuzzy - what responsibility should each have?
        traverse_to_first_leaf is no longer necessary - remove it
    
        cursor functions are too light - it should really be calling most of the tree.c api to do what needs to be done
            tree functions should mainly deal with nodes (which the cursor shouldn't know about)

            abstraction layers: vm -> cursor -> tree -> node

            need to pull some complexity out of tree.c and into cursor.c

        cursor abstraction:
            cursors are created when starting a new transaction, and destroyed when transaction ends (commit or not)
            a cursor is associated with a table AND an ordering (eg, by which column the records are indexed)
            a cursor has a closed and open state:
                <open stmt> opens a cursor
                <close stmt>, <commit stmt>, or <rollback stmt> closes cursor
            cursor may be before first record or one past the last record
            fetch - moves cursor to next record and returns that record
            delete - deletes record pointed to by cursor - moves cursor to next record after deletion
            update - updates records pointed to by cursor

    allocated_memory variable show -52 bytes when runnging sol.sql
        I suspect that a non-wrapper allocation function is being called somewhere
        OR free_w has a wrong size argument

    Probably should cached schema and use that whenever vdbmeta_get_schema is called rather than having to
        manually free it every single time - error prone and just makes everything look messier

    User defined primary keys
        Need to make sure keys are unique
        Need to rewrite B-Tree to not always append at end

    Subqueries

    Add tests for errors:
        insert_invalid_col_name.sql <----should report error message.  Silenty failing now when mistyped, and hard to debug
            create table planets (name string, moons int);
            insert into planets (name, gravity) values ("Earth", 9.81); //this should report error instead of setting moons to null and ignoring 9.81

        *will probably need to introduce runtime errors here (we have tokenizing and parsing(?) errors already)

    single line comments
        use c-style //

    multiline comments
        use c-style /**/

    Don't need to specify column names if inserting all values
        insert into planets values (...), (...);

    If block size is 256 and block header size is 128, it crashes if not enough room to store schema data
        the attribute names take too much space.  Need an data blocks and overflow blocks for colunm names

    Compile AST to bytecode that runs on vm
        How could this work?
            what instructions are needed?
        when inserting, attribute names should be parsed as expressions (being parsed as single tokens now)
            can remove parse_identifier_tuple function then

            insert into planets (name, mass) values ("Mars", 242), ("Venus", 242), ("Earth", 534);

        
    need checks to prevent table manipulation/creation/dropping when database not open - segfaulting now

    need to return error if table doesn't exist when insert/select/update/delete - segfaulting now

    primary keys rework
        user MUST define primary key using 'primary key' keywords
        'auto' keyword will make an autoincrementing column value
        'constraint' can be used to set primary key
        should allow this:
            create table planets (name string, moons int);
            create table planets (name string not null, id int auto);
            create table planets (name string, moons int, primary key

    Concurrency - need to get read/write locks on file
        pager can take care of this

    Transactions - need a way of rolling back changes if transaction fails
        need a way to track undos (a stack???)

    Logging - database should never be in inconsistent state if system fails
        sqlite uses a write-ahead log


    Connect cli and the database tree to get useful data - play with MariaDB or MySQL to see how they print outputs
        > open school
            database school opened
        > select * from students
            +----+------+-----+
            | id | name | age |
            +----+------+-----+
            |  1 | John |  23 |
            |  2 | Kate |  12 |
            |  3 | Timm |  22 |
            +----+------+-----+
            3 rows in set
        > close school
            database school closed
            [what message?]

    Foreign keys to connect two or more tables
        joins will be needed here

    Custom keys
        Unique or not?
        Have to rewrite tree to split/merge based on where inserted (since it will not be in order anymore)

    views

    unique constraint

    not null constraint

    group by

    joins
    
    split into client and server - use tcp sockets to connect client to server
