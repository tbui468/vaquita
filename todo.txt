    Efficiency of dbms can be estimated with number IO operations

    Use gdb to walk through code (Cherno recommended debugger to learn to read code)
        https://www.cs.yale.edu/homes/aspnes/pinewiki/C(2f)Debugging.html
        Try it on the xv6 (or the risc-v version) - would gdb even work with vm?

    https://github.com/firstcontributions/first-contributionsool
        go to this link and do a basic contribution

    Use ChatGPT to help write paper on neural network to learn
        eviction policy for pager - integration of neural networks
        into legacy code

    check out nandgame for website on how to create processor from scratch
        then try to do it using verilog

    Use Yahoo or some other database standard metric to focus on core features

    cstack.github.io for short guide to basics of SQL implementation (not complete)

    Read Edward Sciore's book on implementing a relational database
        Chapter 4: Memory Management (PDF 92)

    Use SDL and Vulkan to visualize stars/planets/moons in galaxy

    sqlite documentation has great explanation of how bytecode works

    handmade.network has cool mini projects (such as barebones OS with only a few thousand lines of C)

    OS
        James Molloy has a good website for OS dev
        use qemu
        use OS dev so that we can set up skeleton project

    Look here for two star databases:
        https://github.com/astronexus/HYG-Database
        One has around 100,000 stars, one has about 200,000 stars/objects and one database has over 2.5 billion(!) stars

    valgrind --leak-check=full --show-leak-kinds=all --log-file="output.txt" -v ./../build/src/vdb sol.sql

    Walkthrough of git source code (cool!) 
        https://fabiensanglard.net/git_code_review/

***********************Documentation*****************************
Example
Major components
Todos

*************************Performance***************

Takes about 1:40 to lex/parse inserting 10,000 records
Takes about 1:40 to execute the AST for 10,000 records

*******************************TODO**************************************************
    If * is parsed, should remove it and fill in projections ExprList with all column names

    aggregate functions (min/max/avg/sum/count)
        apply to the recordsets in vdbcursor_apply_projection

        parser should not parser tokens for projection anymore:
            need to parse an expression:
                name
                avg(mass)
                sum(1) //should be the same as count([attr])
                count(name)
                min(mass)
                max(mass) + 100

    Compile program in release mode
        compare how long it takes to execute query.sql

    'having' keyword
        where cannot be used with aggregates, so having was included
        select name, grade from students group by gender having avg(grade) > 50; ????
        do some examples on pgexercises

    subqueries
        select distinct name from (select * from planets order by name desc);
        what other subqueries could we use as tests?

        Records are tuples of data
        A record is the entire tuple of data
        A tuple is a (possible) subset of the record

    limit
        select * form stars where dist < 1000.0 order by dist asc limit 10;

    ***Big Rewrite***
    Switch to new B-Tree format: have leaves hold ptrs to data blocks (which can now hold arbitrarily large records, so strings can fit)
        This allows multiple indexes in the same file more easily since they can all reference same data blocks
        For now, don't cluster data - just have it appended to currently free data blocks
        Make indexes work correctly with primary key:
            where id = 10, id != 10, id < 10, id > 10 
                How do we optmize cursor postion for these?  Is there a general query plan for all these cases?
                    Where does the cursor start?
                    Where does it end?
                    How many cursors do we need?

            Since indexing only slows down inserts/updates/deletes, we could insert everything, and then
                index ALL columns to speed up select queries

        how should multi-column keys work?  Here's one possible syntax:
            create table planets (id int key, name string, mass int key); //primary key is (id, mass)

    ***Big feature***
    joins
        cross-join
        inner-join (cross join with a condition)
        outer joins

        will need foreign keys here
        

    copy stmt
        *this should require the csv to be cleaned, but it'll be faster than parsing and executing SQL for all the inserts
        copy stars(id, proper, dist) from "hyb_30.csv" delimiter ',' csv header;

    Compiler may make insertion faster
        codegen.c - spits out bytecode
        vm.c - runs vm

    Load up a large database so that buffer pool can be tested
        generate script to:
            create database and table first (drop if existing)
            then add about 10 stars
            then 100
            then 1000
            then 10000
            then entire file (10000s)

    Eviction policy
        make db really big so that the program crashes because of too much memory allocated, then evict pages to fix it
        Or just manually limit number of pages allowed to hold at a time
        Use O_DIRECT to bypass OS pager

        implement LRU policy for now, but make it easy to swap algorithms (such as to clock policy or using a nn)
        make a linked list of currently cached blocks
        define MAX_BLOCKS 4
        when we need a block, scan the list and look for it
        if not found, remove last block (flush if necessary) and load in new block from disk 

    tree.c/cursor (in vdb.c) needs a massive rewrite - it's too confusing now
        start with caching the schema instead of dynamically allocating/freeing it in every tree function
        abstraction between cursor and tree is fuzzy - what responsibility should each have?
        traverse_to_first_leaf is no longer necessary - remove it
    
        cursor functions are too light - it should really be calling most of the tree.c api to do what needs to be done
            tree functions should mainly deal with nodes (which the cursor shouldn't know about)

            abstraction layers: vm -> cursor -> tree -> node

            need to pull some complexity out of tree.c and into cursor.c

        cursor abstraction:
            cursors are created when starting a new transaction, and destroyed when transaction ends (commit or not)
            a cursor is associated with a table AND an ordering (eg, by which column the records are indexed)
            a cursor has a closed and open state:
                <open stmt> opens a cursor
                <close stmt>, <commit stmt>, or <rollback stmt> closes cursor
            cursor may be before first record or one past the last record
            fetch - moves cursor to next record and returns that record
            delete - deletes record pointed to by cursor - moves cursor to next record after deletion
            update - updates records pointed to by cursor

    allocated_memory variable show -52 bytes when runnging sol.sql
        I suspect that a non-wrapper allocation function is being called somewhere
        OR free_w has a wrong size argument

    Probably should cached schema and use that whenever vdbmeta_get_schema is called rather than having to
        manually free it every single time - error prone and just makes everything look messier

    User defined primary keys
        Need to make sure keys are unique
        Need to rewrite B-Tree to not always append at end

    Subqueries

    Add tests for errors:
        insert_invalid_col_name.sql <----should report error message.  Silenty failing now when mistyped, and hard to debug
            create table planets (name string, moons int);
            insert into planets (name, gravity) values ("Earth", 9.81); //this should report error instead of setting moons to null and ignoring 9.81

        *will probably need to introduce runtime errors here (we have tokenizing and parsing(?) errors already)

    single line comments
        use c-style //

    multiline comments
        use c-style /**/

    Don't need to specify column names if inserting all values
        insert into planets values (...), (...);

    If block size is 256 and block header size is 128, it crashes if not enough room to store schema data
        the attribute names take too much space.  Need an data blocks and overflow blocks for colunm names

    Compile AST to bytecode that runs on vm
        How could this work?
            what instructions are needed?
        when inserting, attribute names should be parsed as expressions (being parsed as single tokens now)
            can remove parse_identifier_tuple function then

            insert into planets (name, mass) values ("Mars", 242), ("Venus", 242), ("Earth", 534);

        
    need checks to prevent table manipulation/creation/dropping when database not open - segfaulting now

    need to return error if table doesn't exist when insert/select/update/delete - segfaulting now

    primary keys rework
        user MUST define primary key using 'primary key' keywords
        'auto' keyword will make an autoincrementing column value
        'constraint' can be used to set primary key
        should allow this:
            create table planets (name string, moons int);
            create table planets (name string not null, id int auto);
            create table planets (name string, moons int, primary key

    Concurrency - need to get read/write locks on file
        pager can take care of this

    Transactions - need a way of rolling back changes if transaction fails
        need a way to track undos (a stack???)

    Logging - database should never be in inconsistent state if system fails
        sqlite uses a write-ahead log


    Connect cli and the database tree to get useful data - play with MariaDB or MySQL to see how they print outputs
        > open school
            database school opened
        > select * from students
            +----+------+-----+
            | id | name | age |
            +----+------+-----+
            |  1 | John |  23 |
            |  2 | Kate |  12 |
            |  3 | Timm |  22 |
            +----+------+-----+
            3 rows in set
        > close school
            database school closed
            [what message?]

    Foreign keys to connect two or more tables
        joins will be needed here

    Custom keys
        Unique or not?
        Have to rewrite tree to split/merge based on where inserted (since it will not be in order anymore)

    views

    unique constraint

    not null constraint

    group by

    joins
    
    split into client and server - use tcp sockets to connect client to server
